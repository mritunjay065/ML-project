"""
Script to display comprehensive model training results and comparisons
"""
import pickle
import pandas as pd
import numpy as np
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

def display_model_results():
    """Display comprehensive model training results"""
    
    # Load the best model and preprocessor
    with open('artifacts/model.pkl', 'rb') as f:
        best_model = pickle.load(f)
    
    with open('artifacts/preprocessor.pkl', 'rb') as f:
        preprocessor = pickle.load(f)
    
    # Load test data
    test_df = pd.read_csv('artifacts/test.csv')
    X_test = test_df.drop(['math score'], axis=1)
    y_test = test_df['math score']
    
    # Transform and predict
    X_test_scaled = preprocessor.transform(X_test)
    predictions = best_model.predict(X_test_scaled)
    
    # Calculate metrics
    r2 = r2_score(y_test, predictions)
    mae = mean_absolute_error(y_test, predictions)
    mse = mean_squared_error(y_test, predictions)
    rmse = np.sqrt(mse)
    
    # Display comprehensive results
    print("\n" + "=" * 70)
    print(" " * 15 + "MODEL TRAINING RESULTS")
    print("=" * 70)
    
    print("\nğŸ† BEST MODEL: Linear Regression")
    print("-" * 70)
    
    print("\nğŸ“Š COMPREHENSIVE METRICS:")
    print(f"   â€¢ R2 Score:  {r2:.4f} ({r2*100:.2f}% accuracy)")
    print(f"   â€¢ MAE:       {mae:.4f} (avg prediction error in points)")
    print(f"   â€¢ RMSE:      {rmse:.4f} (penalizes large errors)")
    print(f"   â€¢ MSE:       {mse:.4f}")
    
    print("\n" + "=" * 70)
    print(" " * 10 + "ALL MODELS COMPARISON (Test R2 Scores)")
    print("=" * 70)
    
    # Model comparison data (from training logs)
    models_data = [
        ("Linear Regression", 0.8804, "ğŸ¥‡"),
        ("Gradient Boosting", 0.8748, "ğŸ¥ˆ"),
        ("AdaBoost Regressor", 0.8523, "ğŸ¥‰"),
        ("Random Forest", 0.8534, "  "),
        ("XGBRegressor", 0.8492, "  "),
        ("Decision Tree", 0.7293, "  "),
        ("K-Neighbors Regressor", 0.5197, "  "),
    ]
    
    print("\n{:<25} {:<15} {:<10} {}".format("Model", "R2 Score", "Accuracy", ""))
    print("-" * 70)
    for model_name, score, medal in models_data:
        print(f"{model_name:<25} {score:<15.4f} {score*100:>6.2f}%     {medal}")
    
    print("\n" + "=" * 70)
    print("âœ… All 7 models were trained with automatic hyperparameter tuning")
    print("âœ… GridSearchCV with 3-fold cross-validation")
    print("âœ… Best hyperparameters automatically selected for each model")
    print("=" * 70)
    
    print("\nğŸ“ˆ HYPERPARAMETER TUNING SUMMARY:")
    print("-" * 70)
    print("â€¢ Linear Regression:       No hyperparameters (baseline)")
    print("â€¢ Gradient Boosting:       lr=0.05, n_estimators=128, subsample=0.6")
    print("â€¢ AdaBoost:                lr=0.5, n_estimators=256")
    print("â€¢ Random Forest:           n_estimators=256")
    print("â€¢ XGBoost:                 lr=0.05, n_estimators=64")
    print("â€¢ Decision Tree:           criterion='friedman_mse'")
    print("â€¢ K-Neighbors:             n_neighbors=11")
    print("=" * 70)
    
    print("\nğŸ’¾ SAVED ARTIFACTS:")
    print("-" * 70)
    print("   ğŸ“ artifacts/model.pkl          - Best trained model")
    print("   ğŸ“ artifacts/preprocessor.pkl   - Data preprocessing pipeline")
    print("   ğŸ“ artifacts/train.csv          - Training dataset")
    print("   ğŸ“ artifacts/test.csv           - Testing dataset")
    print("=" * 70 + "\n")

if __name__ == "__main__":
    display_model_results()
